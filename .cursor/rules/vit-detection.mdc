---
description: Vision Transformer (ViT) による人物検出の実装ガイドライン
---

# Vision Transformer 人物検出モジュール

## 概要

[vit_detector.py](mdc:src/vit_detector.py) は、Hugging Face Transformers の DETR または ViT-Det モデルを使用して人物検出を行います。

## 使用モデル

- **DETR**: `facebook/detr-resnet-50` - ResNet バックボーン + Transformer
- **ViT-Det**: `microsoft/vit-det-base` - Pure Vision Transformer

## 主要機能

### 1. モデルロード

```python
from transformers import DetrImageProcessor, DetrForObjectDetection
import torch

class ViTDetector:
    def __init__(self, model_name: str, confidence_threshold: float = 0.5, device: str = "mps"):
        self.model_name = model_name
        self.confidence_threshold = confidence_threshold
        self.device = torch.device(device)
        self.model = None
        self.processor = None

    def load_model(self):
        """事前学習済みViTモデルをロード"""
        self.processor = DetrImageProcessor.from_pretrained(self.model_name)
        self.model = DetrForObjectDetection.from_pretrained(self.model_name)
        self.model.to(self.device)
        self.model.eval()
```

### 2. 前処理

- **パッチ分割**: 画像を 16×16 または 32×32 ピクセルのパッチに分割
- **正規化**: ImageNet 統計で正規化 (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
- **リサイズ**: モデル入力サイズに合わせてリサイズ

```python
def _preprocess(self, frame: np.ndarray) -> torch.Tensor:
    """入力画像の前処理"""
    # OpenCV BGR → RGB変換
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    # Processorによる前処理 (リサイズ、正規化、テンソル化)
    inputs = self.processor(images=frame_rgb, return_tensors="pt")
    return inputs.to(self.device)
```

### 3. 検出処理

```python
def detect(self, frame: np.ndarray) -> List[Detection]:
    """人物検出を実行"""
    inputs = self._preprocess(frame)

    with torch.no_grad():
        outputs = self.model(**inputs)

    # 後処理: バウンディングボックス、信頼度、クラスIDを抽出
    detections = self._postprocess(outputs, frame.shape)

    # 人物クラス(COCO class_id=1)のみをフィルタリング
    person_detections = [d for d in detections if d.class_id == 1]

    # 信頼度閾値でフィルタリング
    filtered = [d for d in person_detections if d.confidence >= self.confidence_threshold]

    return filtered
```

### 4. 後処理

- **座標変換**: 正規化座標 → ピクセル座標
- **NMS (Non-Maximum Suppression)**: 重複検出の除去
- **足元座標計算**: バウンディングボックスの中心下端座標を計算

```python
def _postprocess(self, outputs, image_shape) -> List[Detection]:
    """モデル出力を検出結果に変換"""
    # ロジット → 確率
    probas = outputs.logits.softmax(-1)[0, :, :-1]

    # 信頼度が閾値以上の検出のみ保持
    keep = probas.max(-1).values > self.confidence_threshold

    # バウンディングボックスを画像サイズにスケール
    boxes = outputs.pred_boxes[0, keep]
    # DETR形式 (center_x, center_y, width, height) → (x, y, width, height)
    boxes = self._convert_boxes(boxes, image_shape)

    detections = []
    for box, score, label in zip(boxes, probas[keep], ...):
        # 足元座標を計算
        foot_x = box[0] + box[2] / 2
        foot_y = box[1] + box[3]

        detections.append(Detection(
            bbox=tuple(box),
            confidence=float(score.max()),
            class_id=int(label.argmax()),
            class_name="person",
            camera_coords=(foot_x, foot_y)
        ))

    return detections
```

### 5. Attention Map 可視化

デバッグモードで ViT の注意機構を可視化:

```python
def get_attention_map(self, frame: np.ndarray) -> np.ndarray:
    """Attention Mapを取得 (可視化用)"""
    inputs = self._preprocess(frame)

    with torch.no_grad():
        outputs = self.model(**inputs, output_attentions=True)

    # 最終層のAttention weightを取得
    attentions = outputs.attentions[-1]  # [batch, heads, seq_len, seq_len]

    # 平均化して2D mapに変換
    attention_map = attentions[0].mean(0).cpu().numpy()

    return attention_map
```

### 6. バッチ推論

効率化のため複数フレームを同時処理:

```python
def detect_batch(self, frames: List[np.ndarray]) -> List[List[Detection]]:
    """バッチ推論"""
    # 複数フレームをバッチ化
    inputs = self.processor(images=[cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in frames],
                           return_tensors="pt").to(self.device)

    with torch.no_grad():
        outputs = self.model(**inputs)

    # 各フレームの検出結果を個別に処理
    all_detections = []
    for i, frame in enumerate(frames):
        detections = self._postprocess_single(outputs, i, frame.shape)
        all_detections.append(detections)

    return all_detections
```

## パフォーマンス最適化

- **MPS 使用**: Apple Silicon GPU で高速化
- **Flash Attention**: PyTorch 2.0+の最適化された Attention 実装
- **半精度演算**: メモリ削減（オプション、精度とのトレードオフ）
- **バッチサイズ調整**: [config.yaml](mdc:config.yaml) の `detection.batch_size` で調整

## COCO クラス ID

- **Person**: class_id = 1 (このプロジェクトで検出対象)
- 他のクラスは除外する

## 設定項目 (config.yaml)

```yaml
detection:
  model_name: "facebook/detr-resnet-50"
  confidence_threshold: 0.5
  nms_threshold: 0.4
  device: "mps" # mps, cuda, cpu
  batch_size: 4
  patch_size: 16
```
